#!/usr/bin/env python3
"""
Submit text to be annotated to PubTator API
"""
import json
import os
import time
from typing import Optional

import bioc
import requests  # type: ignore
from belb.preprocessing.data import Entities
from belb.resources import Corpora
from bioc import biocjson, biocxml
from loguru import logger
from unidecode import unidecode

from models.base import Model, parse_args

# pylint: disable=singleton-comparison

SLEEP_TIME = 5
BATCH_ABSTRACTS = 100
BATCH_FULL_TEXT = 5

PUBTATOR_ENTITY_TYPES = {Entities.CELL_LINE: "Cellline"}


def split_into_chunks(lst, n):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i : i + n]


class PubTatorApi(Model):
    """
    Wrapper to fetch annotations generated by PubTator models
    """

    @property
    def corpora(self):

        return [(Corpora.BIOID.name, Entities.CELL_LINE)]

    def submit_requests(self, input_text: str, entity_type: str):
        """
        Submit text to be annotated to PubTator API
        """

        if entity_type not in set(PUBTATOR_ENTITY_TYPES.values()):
            raise ValueError(
                f"Invalid entity_type {entity_type}! Supports only {set(PUBTATOR_ENTITY_TYPES.values())}."
            )

        input_text = unidecode(input_text)

        url = f"https://www.ncbi.nlm.nih.gov/research/pubtator-api/annotations/annotate/submit/{entity_type}"

        # submit request
        r = requests.post(url, data=input_text.encode("utf-8"))
        if r.status_code != 200:
            raise ValueError("[Error]: HTTP code " + str(r.status_code))

        session_id = r.text

        return session_id

    def create_input(self):
        """
        Create input for GNormPlus
        """

        for corpus_name, _ in self.corpora:

            path = os.path.join(self.in_dir, "input", corpus_name)

            sessions = {}

            if not os.path.exists(path):

                os.makedirs(path, exist_ok=True)

                corpus_path, corpus_config = self.get_corpus(
                    name=corpus_name,
                    entity_type=self.entity_type,
                    sentences=self.sentences,
                )

                split_path = os.path.join(corpus_path, f"{self.split}.bioc.json")

                with open(split_path) as fp:
                    documents = self.convert_documents(
                        collection=biocjson.load(fp), output_format="bioc"
                    )

                chunksize = BATCH_FULL_TEXT if corpus_config["pmc"] else BATCH_ABSTRACTS

                for idx, batch in enumerate(split_into_chunks(documents, chunksize)):

                    file_name = (f"{corpus_name}_{self.split}_batch{idx}.bioc",)

                    path = os.path.join(self.in_dir, "input", corpus_name, file_name)

                    with open(path, "w") as fp:
                        collection = bioc.BioCCollection()
                        collection.documents = batch
                        biocxml.dump(collection, fp)

                    input_text = ""
                    with open(path) as fp:
                        for line in fp:
                            input_text += unidecode(line)

                    session_id = self.submit_requests(
                        input_text=input_text,
                        entity_type=PUBTATOR_ENTITY_TYPES[self.entity_type],
                    )

                    logger.info(
                        "Submitted {} documents in `{}`. Wait {} seconds before new submission...",
                        len(batch),
                        path,
                        SLEEP_TIME,
                    )

                    time.sleep(SLEEP_TIME)

                    sessions[session_id] = file_name

            with open(os.path.join(self.in_dir, "sessions.json"), "w") as fp:
                json.dump(sessions, fp)

    def retrieve_session(self, session_no: str) -> Optional[str]:
        """
        Retrive text annotated via PubTator API
        """
        r = requests.get(
            f"https://www.ncbi.nlm.nih.gov/research/pubtator-api/annotations/annotate/retrieve/{session_no}"
        )

        text = None
        if r.status_code == 200:
            text = r.text
        elif r.status_code == 404:
            msg = f"{session_no}: Result is not ready. [{r.status_code}]"
            if "Error 501" in r.content.decode("utf-8"):
                msg += "Response content: The server does not support the action requested by the browser."

            logger.info(msg)
        return text

    def parse_output(self, corpus_name: str, entity_type: str, gold: dict):

        sessions_path = os.path.join(self.in_dir, "sessions.json")

        with open(sessions_path) as fp:
            sessions = json.load(fp)

        breakpoint()


def main():
    """
    Script
    """

    args = parse_args()

    db_conifg = os.path.join(os.getcwd(), "data", "config", "db.yaml")

    pubtator_api = PubTatorApi(
        directory=args.dir,
        belb_directory=args.belb,
        db_config=db_conifg,
        split=args.split,
        joint_ner_nen=True,
        obsolete_kb=True,
    )

    if args.run == "input":
        pubtator_api.create_input()

    elif args.run == "output":
        results_dir = os.path.join(os.getcwd(), "data", "baselines", "pubtator_api")
        os.makedirs(results_dir, exist_ok=True)
        results = pubtator_api.collect_results()
        with open(os.path.join(results_dir, "results.json"), "w") as fp:
            json.dump(results, fp)


if __name__ == "__main__":
    main()
